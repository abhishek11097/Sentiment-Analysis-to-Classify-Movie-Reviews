{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def dataset(data,label,dir):\n",
    "    a=os.listdir(dir)\n",
    "    for x in a:\n",
    "        x = dir + \"/\" + x\n",
    "        f=open(x,\"r\")\n",
    "        s=f.read()\n",
    "        data.append(s)\n",
    "        if(dir==\"neg\"):\n",
    "            label.append(0)\n",
    "        else:\n",
    "            label.append(1)    \n",
    "    return data,label  \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "label=[]\n",
    "data,label=dataset(data,label,\"neg\")\n",
    "data,label=dataset(data,label,\"pos\")\n",
    "print(len(data))\n",
    "print(len(label))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "i=0\n",
    "t= nltk.tokenize.TreebankWordTokenizer()\n",
    "s=nltk.stem.PorterStemmer()\n",
    "to=nltk.stem.WordNetLemmatizer()\n",
    "for text in data:    \n",
    "    text = text.decode(\"utf8\")\n",
    "    tokens=t.tokenize(text) \n",
    "    le=\" \".join(to.lemmatize(token) for token in tokens)\n",
    "    tokens=t.tokenize(le) \n",
    "    data[i]=\" \".join(s.stem(token) for token in tokens)\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as tf\n",
    "tfidf = tf(min_df=0.1,max_df=0.7,ngram_range=(1,4))\n",
    "features=tfidf.fit_transform(data)\n",
    "df=pd.DataFrame(features.todense(),columns=tfidf.get_feature_names())\n",
    "df=df.values\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "gnb1 = svm.SVC()\n",
    "gnb2 = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "gnb3 = linear_model.SGDClassifier()\n",
    "gnb.fit(df,label)\n",
    "gnb1.fit(df,label)\n",
    "gnb2.fit(df,label)\n",
    "gnb3.fit(df,label)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text=\"I wish I knew what to make of a movie like this. It seems to be divided into two parts -- action sequences and personal dramas ashore. It follows Ashton Kutsher through survival swimmer school, guided by Master Chief Kevin Costner, then to Alaska where a couple of spectacular rescues take place, the last resulting in death.<br /><br />I must say that the scenes on the beach struck me as so stereotypical in so many ways that they should be barnacle encrusted. A typical bar room fight between Navy guys and Coast Guardsmen (puddle pirates). The experienced old timer Costner who is, as an elderly bar tender tells him, married to the Coast Guard. The older chief who keeps trying to prove to himself that he's still nineteen. The neglected ex wife ashore to whom Kostner pays a farewell visit. The seemingly sadistic demands placed on the swimmers by the instructors, all in pursuit of a loftier goal. The gifted young man hobbled by a troubled past.<br /><br />The problem is that we've seen it all before. If it's Kevin Costner here, it's Clint Eastwood or John Wayne or Lou Gosset Jr. or Vigo Mortenson or Robert DeNiro elsewhere. And the climactic scene has elements drawn shamelessly from The Perfect Storm and Dead Calm. None of it is fresh and none of the old stereotyped characters and situations are handled with any originality.<br /><br />It works best as a kind of documentary of what goes on in the swimmer's school and what could happen afterward and even that's a little weak because we don't get much in the way of instruction. It's mostly personal conflict, romance, and tension about washing out.<br /><br />It's a shame because the U. S. Coast Guard is rather a noble outfit, its official mission being the safety of lives and property at sea. In war time it is transferred to the Navy Department and serves in combat roles. In World War II, the Coast Guard even managed to have a Medal of Honor winner in its ranks.<br /><br />But, again, we don't learn much about that. We don't really learn much about anything. The film devolves into a succession of visual displays and not too much else. A disappointment.\"\n",
    "#t= nltk.tokenize.TreebankWordTokenizer()\n",
    "#s=nltk.stem.PorterStemmer()#\n",
    "#to=nltk.stem.WordNetLemmatizer()\n",
    "#text = text.decode(\"utf8\")\n",
    "#tokens=t.tokenize(text) \n",
    "#le=\" \".join(to.lemmatize(token) for token in tokens)\n",
    "#tokens=t.tokenize(le) \n",
    "#d=\" \".join(s.stem(token) for token in tokens) \n",
    "#print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "a,b=dataset(a,b,\"posTest\")   \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "i=0\n",
    "t= nltk.tokenize.TreebankWordTokenizer()\n",
    "s=nltk.stem.PorterStemmer()\n",
    "to=nltk.stem.WordNetLemmatizer()\n",
    "for text in a:    \n",
    "    text=text.decode('utf-8')\n",
    "    tokens=t.tokenize(text) \n",
    "    le=\" \".join(to.lemmatize(token) for token in tokens)\n",
    "    tokens=t.tokenize(le) \n",
    "    a[i]=\" \".join(s.stem(token) for token in tokens)\n",
    "    i+=1\n",
    "print(\"done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "counter=0\n",
    "for d in a:\n",
    "    q=tfidf.transform([d])\n",
    "    q=q.todense()\n",
    "    q=q[0]\n",
    "    q=q.tolist()\n",
    "    q=q[0]\n",
    "    s=gnb.predict([q])\n",
    "    counter+=1\n",
    "    if(s==0):\n",
    "        count+=1\n",
    "print(count)\n",
    "print(counter)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3157\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count=0\n",
    "counter=0\n",
    "for d in a:\n",
    "    q=tfidf.transform([d])\n",
    "    q=q.todense()\n",
    "    q=q[0]\n",
    "    q=q.tolist()\n",
    "    q=q[0]\n",
    "    s=gnb1.predict([q])\n",
    "    counter+=1\n",
    "    if(s==0):\n",
    "        count+=1\n",
    "print(count)\n",
    "print(counter)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3333\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "counter=0\n",
    "for d in a:\n",
    "    q=tfidf.transform([d])\n",
    "    q=q.todense()\n",
    "    q=q[0]\n",
    "    q=q.tolist()\n",
    "    q=q[0]\n",
    "    s=gnb2.predict([q])\n",
    "    counter+=1\n",
    "    if(s==0):\n",
    "        count+=1\n",
    "print(count)\n",
    "print(counter)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "counter=0\n",
    "for d in a:\n",
    "    q=tfidf.transform([d])\n",
    "    q=q.todense()\n",
    "    q=q[0]\n",
    "    q=q.tolist()\n",
    "    q=q[0]\n",
    "    s=gnb3.predict([q])\n",
    "    counter+=1\n",
    "    if(s==0):\n",
    "        count+=1\n",
    "print(count)\n",
    "print(counter)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
